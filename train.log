nohup: ÂøΩÁï•ËæìÂÖ•
no change     /home/yhj/miniconda3/condabin/conda
no change     /home/yhj/miniconda3/bin/conda
no change     /home/yhj/miniconda3/bin/conda-env
no change     /home/yhj/miniconda3/bin/activate
no change     /home/yhj/miniconda3/bin/deactivate
no change     /home/yhj/miniconda3/etc/profile.d/conda.sh
no change     /home/yhj/miniconda3/etc/fish/conf.d/conda.fish
no change     /home/yhj/miniconda3/shell/condabin/Conda.psm1
no change     /home/yhj/miniconda3/shell/condabin/conda-hook.ps1
no change     /home/yhj/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /home/yhj/miniconda3/etc/profile.d/conda.csh
no change     /home/yhj/.bashrc
No action taken.

CondaError: Run 'conda init' before 'conda activate'

[2025-11-26 15:14:05] INFO (torcheeg/MainThread) üîç | Detected cached processing results, reading cache from ./cached_data/thubenchmark_EA.
[2025-11-26 15:14:06] INFO (torcheeg/MainThread) üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/kfold_split.
[2025-11-26 15:14:06] INFO (torcheeg/MainThread) üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
[2025-11-26 15:14:06] INFO (torcheeg/MainThread) üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/test_val_split_0.
[2025-11-26 15:14:06] INFO (torcheeg/MainThread) üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
/home/yhj/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/yhj/pythonProject/EEGAP/train.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model.load_state_dict(torch.load(f'./checkpoints/{args.dataset}_{args.model}_{args.seed}_fold{index}_best.pth'))
[2025-11-26 15:20:08] INFO (torcheeg/MainThread) üìä | Create the split of train and test set.
[2025-11-26 15:20:08] INFO (torcheeg/MainThread) üòä | Please set [92msplit_path[0m to [92m./cached_data/thubenchmark_split/test_val_split_1[0m for the next run, if you want to use the same setting for the experiment.
/home/yhj/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/yhj/pythonProject/EEGAP/train.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model.load_state_dict(torch.load(f'./checkpoints/{args.dataset}_{args.model}_{args.seed}_fold{index}_best.pth'))
[2025-11-26 15:27:18] INFO (torcheeg/MainThread) üìä | Create the split of train and test set.
[2025-11-26 15:27:18] INFO (torcheeg/MainThread) üòä | Please set [92msplit_path[0m to [92m./cached_data/thubenchmark_split/test_val_split_2[0m for the next run, if you want to use the same setting for the experiment.
/home/yhj/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/yhj/pythonProject/EEGAP/train.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model.load_state_dict(torch.load(f'./checkpoints/{args.dataset}_{args.model}_{args.seed}_fold{index}_best.pth'))
[2025-11-26 15:35:37] INFO (torcheeg/MainThread) üìä | Create the split of train and test set.
[2025-11-26 15:35:37] INFO (torcheeg/MainThread) üòä | Please set [92msplit_path[0m to [92m./cached_data/thubenchmark_split/test_val_split_3[0m for the next run, if you want to use the same setting for the experiment.
/home/yhj/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/yhj/pythonProject/EEGAP/train.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model.load_state_dict(torch.load(f'./checkpoints/{args.dataset}_{args.model}_{args.seed}_fold{index}_best.pth'))
[2025-11-26 15:41:12] INFO (torcheeg/MainThread) üìä | Create the split of train and test set.
[2025-11-26 15:41:12] INFO (torcheeg/MainThread) üòä | Please set [92msplit_path[0m to [92m./cached_data/thubenchmark_split/test_val_split_4[0m for the next run, if you want to use the same setting for the experiment.
/home/yhj/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/home/yhj/pythonProject/EEGAP/train.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  best_model.load_state_dict(torch.load(f'./checkpoints/{args.dataset}_{args.model}_{args.seed}_fold{index}_best.pth'))
[2025-11-26 15:48:05] INFO (torcheeg/MainThread) üîç | Detected cached processing results, reading cache from ./cached_data/thubenchmark_EA.
[2025-11-26 15:48:06] INFO (torcheeg/MainThread) üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/kfold_split.
[2025-11-26 15:48:06] INFO (torcheeg/MainThread) üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
[2025-11-26 15:48:06] INFO (torcheeg/MainThread) üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/test_val_split_0.
[2025-11-26 15:48:06] INFO (torcheeg/MainThread) üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
/home/yhj/miniconda3/envs/torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
