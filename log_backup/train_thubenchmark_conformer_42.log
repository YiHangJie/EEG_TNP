2025-11-26 16:08:05 | INFO | root | Training thubenchmark with conformer
2025-11-26 16:08:05 | INFO | root | Namespace(dataset='thubenchmark', model='conformer', epochs=400, batch_size=128, lr=0.001, weight_decay=0.0001, patience=20, seed=42, gpu_id=0)
2025-11-26 16:08:05 | INFO | torcheeg | üîç | Detected cached processing results, reading cache from ./cached_data/thubenchmark_EA.
2025-11-26 16:08:06 | INFO | root | Dataset: thubenchmark, Sample_num: 8400, num_classes: 40
2025-11-26 16:08:06 | INFO | torcheeg | üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/kfold_split.
2025-11-26 16:08:06 | INFO | torcheeg | üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
2025-11-26 16:08:06 | INFO | root | sample num in train set: 6720, sample num in test set: 1680
2025-11-26 16:08:06 | INFO | root | Model: conformer, Parameter Num: 1195880
2025-11-26 16:08:06 | INFO | torcheeg | üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/test_val_split_0.
2025-11-26 16:08:06 | INFO | torcheeg | üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
2025-11-26 16:08:06 | INFO | root | sample num in train set: 6720, sample num in val set: 840, sample num in test set: 840
2025-11-26 16:08:30 | INFO | root | Epoch: 1, Train Loss: 3.7313, Val Acc: 0.0250, Val Loss: 3.7039, Test Acc: 0.0190, Test Loss: 3.7000, Learning Rate: 0.001000
2025-11-26 16:08:53 | INFO | root | Epoch: 2, Train Loss: 3.6337, Val Acc: 0.0381, Val Loss: 3.4892, Test Acc: 0.0333, Test Loss: 3.5146, Learning Rate: 0.001000
2025-11-26 16:09:16 | INFO | root | Epoch: 3, Train Loss: 3.3884, Val Acc: 0.0571, Val Loss: 3.1572, Test Acc: 0.0643, Test Loss: 3.1542, Learning Rate: 0.001000
2025-11-26 16:09:39 | INFO | root | Epoch: 4, Train Loss: 3.1985, Val Acc: 0.0524, Val Loss: 3.1170, Test Acc: 0.0452, Test Loss: 3.1535, Learning Rate: 0.001000
2025-11-26 16:10:02 | INFO | root | Epoch: 5, Train Loss: 3.1264, Val Acc: 0.0690, Val Loss: 3.0083, Test Acc: 0.0786, Test Loss: 3.0220, Learning Rate: 0.001000
2025-11-26 16:10:25 | INFO | root | Epoch: 6, Train Loss: 3.0704, Val Acc: 0.0655, Val Loss: 2.9628, Test Acc: 0.0762, Test Loss: 3.0074, Learning Rate: 0.001000
2025-11-26 16:10:49 | INFO | root | Epoch: 7, Train Loss: 2.8965, Val Acc: 0.1190, Val Loss: 2.5994, Test Acc: 0.1250, Test Loss: 2.6327, Learning Rate: 0.001000
2025-11-26 16:11:12 | INFO | root | Epoch: 8, Train Loss: 2.7243, Val Acc: 0.1476, Val Loss: 2.4506, Test Acc: 0.1726, Test Loss: 2.4556, Learning Rate: 0.001000
2025-11-26 16:11:35 | INFO | root | Epoch: 9, Train Loss: 2.5730, Val Acc: 0.1917, Val Loss: 2.3805, Test Acc: 0.2000, Test Loss: 2.4084, Learning Rate: 0.001000
2025-11-26 16:11:58 | INFO | root | Epoch: 10, Train Loss: 2.5155, Val Acc: 0.1750, Val Loss: 2.2941, Test Acc: 0.1726, Test Loss: 2.3650, Learning Rate: 0.001000
2025-11-26 16:12:21 | INFO | root | Epoch: 11, Train Loss: 2.4941, Val Acc: 0.1988, Val Loss: 2.2557, Test Acc: 0.2036, Test Loss: 2.2721, Learning Rate: 0.001000
2025-11-26 16:12:44 | INFO | root | Epoch: 12, Train Loss: 2.5003, Val Acc: 0.2476, Val Loss: 2.2471, Test Acc: 0.2440, Test Loss: 2.2594, Learning Rate: 0.001000
2025-11-26 16:13:08 | INFO | root | Epoch: 13, Train Loss: 2.4694, Val Acc: 0.2214, Val Loss: 2.3142, Test Acc: 0.2274, Test Loss: 2.3383, Learning Rate: 0.001000
2025-11-26 16:13:31 | INFO | root | Epoch: 14, Train Loss: 2.4808, Val Acc: 0.1952, Val Loss: 2.3037, Test Acc: 0.2214, Test Loss: 2.3355, Learning Rate: 0.001000
2025-11-26 16:13:54 | INFO | root | Epoch: 15, Train Loss: 2.4833, Val Acc: 0.2357, Val Loss: 2.3116, Test Acc: 0.2583, Test Loss: 2.3114, Learning Rate: 0.001000
2025-11-26 16:14:17 | INFO | root | Epoch: 16, Train Loss: 2.4655, Val Acc: 0.1619, Val Loss: 2.4023, Test Acc: 0.1726, Test Loss: 2.4492, Learning Rate: 0.001000
2025-11-26 16:14:40 | INFO | root | Epoch: 17, Train Loss: 2.5184, Val Acc: 0.2119, Val Loss: 2.2991, Test Acc: 0.2464, Test Loss: 2.3423, Learning Rate: 0.001000
2025-11-26 16:15:03 | INFO | root | Epoch: 18, Train Loss: 2.5372, Val Acc: 0.1607, Val Loss: 2.4394, Test Acc: 0.1560, Test Loss: 2.5031, Learning Rate: 0.001000
2025-11-26 16:15:27 | INFO | root | Epoch: 19, Train Loss: 2.5155, Val Acc: 0.1690, Val Loss: 2.4435, Test Acc: 0.1774, Test Loss: 2.5012, Learning Rate: 0.001000
2025-11-26 16:15:50 | INFO | root | Epoch: 20, Train Loss: 2.5224, Val Acc: 0.2250, Val Loss: 2.3464, Test Acc: 0.1881, Test Loss: 2.3792, Learning Rate: 0.001000
2025-11-26 16:16:13 | INFO | root | Epoch: 21, Train Loss: 2.5588, Val Acc: 0.2476, Val Loss: 2.3562, Test Acc: 0.2476, Test Loss: 2.3897, Learning Rate: 0.001000
2025-11-26 16:16:36 | INFO | root | Epoch: 22, Train Loss: 2.5158, Val Acc: 0.2643, Val Loss: 2.3563, Test Acc: 0.2310, Test Loss: 2.3839, Learning Rate: 0.001000
2025-11-26 16:16:59 | INFO | root | Epoch: 23, Train Loss: 2.5170, Val Acc: 0.1774, Val Loss: 2.4562, Test Acc: 0.1524, Test Loss: 2.4683, Learning Rate: 0.000500
2025-11-26 16:17:22 | INFO | root | Epoch: 24, Train Loss: 2.4471, Val Acc: 0.2631, Val Loss: 2.3122, Test Acc: 0.2143, Test Loss: 2.3483, Learning Rate: 0.000500
2025-11-26 16:17:46 | INFO | root | Epoch: 25, Train Loss: 2.4546, Val Acc: 0.2024, Val Loss: 2.4458, Test Acc: 0.1726, Test Loss: 2.4819, Learning Rate: 0.000500
2025-11-26 16:18:09 | INFO | root | Epoch: 26, Train Loss: 2.4651, Val Acc: 0.2583, Val Loss: 2.2789, Test Acc: 0.2333, Test Loss: 2.3097, Learning Rate: 0.000500
2025-11-26 16:18:32 | INFO | root | Epoch: 27, Train Loss: 2.4528, Val Acc: 0.2333, Val Loss: 2.3103, Test Acc: 0.2238, Test Loss: 2.3603, Learning Rate: 0.000500
2025-11-26 16:18:55 | INFO | root | Epoch: 28, Train Loss: 2.4958, Val Acc: 0.2548, Val Loss: 2.3425, Test Acc: 0.2286, Test Loss: 2.3673, Learning Rate: 0.000500
2025-11-26 16:19:18 | INFO | root | Epoch: 29, Train Loss: 2.4565, Val Acc: 0.1774, Val Loss: 2.4496, Test Acc: 0.2036, Test Loss: 2.5057, Learning Rate: 0.000500
2025-11-26 16:19:41 | INFO | root | Epoch: 30, Train Loss: 2.5368, Val Acc: 0.2274, Val Loss: 2.3395, Test Acc: 0.2167, Test Loss: 2.3698, Learning Rate: 0.000500
2025-11-26 16:20:04 | INFO | root | Epoch: 31, Train Loss: 2.4684, Val Acc: 0.2607, Val Loss: 2.3083, Test Acc: 0.2476, Test Loss: 2.3334, Learning Rate: 0.000500
2025-11-26 16:20:28 | INFO | root | Epoch: 32, Train Loss: 2.4996, Val Acc: 0.1774, Val Loss: 2.4601, Test Acc: 0.1833, Test Loss: 2.5068, Learning Rate: 0.000500
2025-11-26 16:20:28 | INFO | root | Early stopping at epoch 32 (no improvement in 20 epochs). Best Val Loss: 2.2471
2025-11-26 16:20:28 | INFO | root | Test Acc: 0.2440, Test Loss: 2.2594
2025-11-26 16:20:28 | INFO | root | sample num in train set: 6720, sample num in test set: 1680
2025-11-26 16:20:28 | INFO | root | Model: conformer, Parameter Num: 1195880
2025-11-26 16:20:28 | INFO | torcheeg | üìä | Detected existing split of train and test set, use existing split from ./cached_data/thubenchmark_split/test_val_split_1.
2025-11-26 16:20:28 | INFO | torcheeg | üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
2025-11-26 16:20:28 | INFO | root | sample num in train set: 6720, sample num in val set: 840, sample num in test set: 840
