2026-01-05 13:38:30 | INFO | root | Training seediv with eegnet
2026-01-05 13:38:30 | INFO | root | Namespace(dataset='seediv', model='eegnet', at_strategy='madry', epsilon=0.1, pgd_step_size=0.01, pgd_steps=20, fbf_replays=2, trades_beta=0.1, clean_ratio=0.0, clip_min=None, clip_max=None, pgd_random_start=True, epochs=400, batch_size=128, lr=0.001, weight_decay=0, patience=20, seed=42, gpu_id=3)
2026-01-05 13:38:30 | INFO | root | AT config | strategy: madry, eps: 0.1, step size: 0.01, steps: 20, fbf_replays: 2, trades_beta: 0.1, clean_ratio: 0.0, random_start: True, clip_min/max: (None, None)
2026-01-05 13:38:30 | INFO | torcheeg | üîç | Detected cached processing results, reading cache from ./cached_data/seediv_EA.
2026-01-05 13:38:45 | INFO | root | Dataset: seediv, Sample_num: 37575, num_classes: 4
2026-01-05 13:38:45 | INFO | torcheeg | üìä | Detected existing split of train and test set, use existing split from ./cached_data/seediv_split/kfold_split.
2026-01-05 13:38:45 | INFO | torcheeg | üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
2026-01-05 13:38:45 | INFO | root | sample num in train set: 30060, sample num in test set: 7515
2026-01-05 13:38:45 | INFO | root | Model: eegnet, Parameter Num: 4080
2026-01-05 13:38:45 | INFO | torcheeg | üìä | Detected existing split of train and test set, use existing split from ./cached_data/seediv_split/test_val_split_0.
2026-01-05 13:38:45 | INFO | torcheeg | üí° | If the dataset is re-generated, you need to re-generate the split of the dataset instead of using the previous split.
2026-01-05 13:38:45 | INFO | root | sample num in train set: 30060, sample num in val set: 3757, sample num in test set: 3758
2026-01-05 13:48:51 | INFO | root | Epoch: 1, Train Loss: 1.4814, Val Acc: 0.2595, Val Loss: 1.3847, Test Acc: 0.2775, Test Loss: 1.3847, Learning Rate: 0.001000
2026-01-05 14:06:22 | INFO | root | Epoch: 2, Train Loss: 1.3921, Val Acc: 0.2805, Val Loss: 1.3803, Test Acc: 0.2618, Test Loss: 1.3826, Learning Rate: 0.001000
2026-01-05 14:25:03 | INFO | root | Epoch: 3, Train Loss: 1.3891, Val Acc: 0.2805, Val Loss: 1.3802, Test Acc: 0.2618, Test Loss: 1.3829, Learning Rate: 0.001000
2026-01-05 14:52:19 | INFO | root | Epoch: 4, Train Loss: 1.3872, Val Acc: 0.2805, Val Loss: 1.3806, Test Acc: 0.2618, Test Loss: 1.3827, Learning Rate: 0.001000
2026-01-05 15:09:37 | INFO | root | Epoch: 5, Train Loss: 1.3861, Val Acc: 0.2803, Val Loss: 1.3802, Test Acc: 0.2621, Test Loss: 1.3827, Learning Rate: 0.001000
2026-01-05 15:19:37 | INFO | root | Epoch: 6, Train Loss: 1.3865, Val Acc: 0.2731, Val Loss: 1.3808, Test Acc: 0.2743, Test Loss: 1.3824, Learning Rate: 0.001000
2026-01-05 15:27:49 | INFO | root | Epoch: 7, Train Loss: 1.3856, Val Acc: 0.2731, Val Loss: 1.3809, Test Acc: 0.2743, Test Loss: 1.3829, Learning Rate: 0.001000
2026-01-05 15:35:59 | INFO | root | Epoch: 8, Train Loss: 1.3856, Val Acc: 0.2731, Val Loss: 1.3813, Test Acc: 0.2743, Test Loss: 1.3823, Learning Rate: 0.001000
2026-01-05 15:44:28 | INFO | root | Epoch: 9, Train Loss: 1.3847, Val Acc: 0.2731, Val Loss: 1.3811, Test Acc: 0.2743, Test Loss: 1.3824, Learning Rate: 0.001000
2026-01-05 15:52:27 | INFO | root | Epoch: 10, Train Loss: 1.3844, Val Acc: 0.2731, Val Loss: 1.3813, Test Acc: 0.2743, Test Loss: 1.3825, Learning Rate: 0.001000
2026-01-05 16:02:15 | INFO | root | Epoch: 11, Train Loss: 1.3840, Val Acc: 0.2731, Val Loss: 1.3820, Test Acc: 0.2743, Test Loss: 1.3830, Learning Rate: 0.001000
2026-01-05 16:15:08 | INFO | root | Epoch: 12, Train Loss: 1.3835, Val Acc: 0.2731, Val Loss: 1.3829, Test Acc: 0.2743, Test Loss: 1.3835, Learning Rate: 0.001000
2026-01-05 16:24:29 | INFO | root | Epoch: 13, Train Loss: 1.3831, Val Acc: 0.2731, Val Loss: 1.3864, Test Acc: 0.2743, Test Loss: 1.3865, Learning Rate: 0.001000
2026-01-05 16:36:21 | INFO | root | Epoch: 14, Train Loss: 1.3828, Val Acc: 0.2731, Val Loss: 1.3857, Test Acc: 0.2743, Test Loss: 1.3858, Learning Rate: 0.000500
2026-01-05 16:54:44 | INFO | root | Epoch: 15, Train Loss: 1.3818, Val Acc: 0.2731, Val Loss: 1.3854, Test Acc: 0.2743, Test Loss: 1.3852, Learning Rate: 0.000500
2026-01-05 17:04:46 | INFO | root | Epoch: 16, Train Loss: 1.3819, Val Acc: 0.2731, Val Loss: 1.3892, Test Acc: 0.2743, Test Loss: 1.3879, Learning Rate: 0.000500
2026-01-05 17:15:55 | INFO | root | Epoch: 17, Train Loss: 1.3819, Val Acc: 0.2731, Val Loss: 1.3881, Test Acc: 0.2743, Test Loss: 1.3868, Learning Rate: 0.000500
2026-01-05 17:26:53 | INFO | root | Epoch: 18, Train Loss: 1.3818, Val Acc: 0.2731, Val Loss: 1.3884, Test Acc: 0.2743, Test Loss: 1.3872, Learning Rate: 0.000500
2026-01-05 17:38:12 | INFO | root | Epoch: 19, Train Loss: 1.3813, Val Acc: 0.2731, Val Loss: 1.3879, Test Acc: 0.2743, Test Loss: 1.3866, Learning Rate: 0.000500
2026-01-05 17:48:54 | INFO | root | Epoch: 20, Train Loss: 1.3816, Val Acc: 0.2731, Val Loss: 1.3878, Test Acc: 0.2743, Test Loss: 1.3869, Learning Rate: 0.000500
2026-01-05 17:59:43 | INFO | root | Epoch: 21, Train Loss: 1.3810, Val Acc: 0.2731, Val Loss: 1.3904, Test Acc: 0.2743, Test Loss: 1.3882, Learning Rate: 0.000500
2026-01-05 18:12:16 | INFO | root | Epoch: 22, Train Loss: 1.3816, Val Acc: 0.2731, Val Loss: 1.3875, Test Acc: 0.2743, Test Loss: 1.3862, Learning Rate: 0.000500
2026-01-05 18:25:31 | INFO | root | Epoch: 23, Train Loss: 1.3816, Val Acc: 0.2731, Val Loss: 1.3893, Test Acc: 0.2743, Test Loss: 1.3878, Learning Rate: 0.000500
2026-01-05 18:25:31 | INFO | root | Early stopping at epoch 23 (no improvement in 20 epochs). Best Val Loss: 1.3802
2026-01-05 18:25:47 | INFO | root | Test Acc: 0.2618, Test Loss: 1.3829
2026-01-05 18:25:47 | INFO | root | Acc: 0.2618¬±0.0000, Loss: 1.3829¬±0.0000
